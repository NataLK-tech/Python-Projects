# -*- coding: utf-8 -*-
"""1. Data preparation (Kazakevych)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ye-KU4wXsTKDxVDgFUKrn0fw40Zh2n5j
"""

import datetime
from datetime import time, timedelta
import pickle
import re
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

warnings.filterwarnings('ignore')

"""# Вспомогательные функции"""

class InfoDataSet:
    def __init__(self, name, data):
        self.name = name
        self.data = data

    def pivot_info(self): # сводная описательная таблица датасета
        print('Name of the data: ', self.name)

        type_values = pd.Series({column: self.data[column].dtypes for column in self.data.columns})
        unique_values = pd.Series({column: self.data[column].unique() for column in self.data.columns})
        pivot_info = pd.DataFrame({'count': self.data.count(),
                                       'null': self.data.isnull().sum(),
                                       'unique': self.data.nunique(),
                                       'type': type_values,
                                       'unique_values': unique_values
                                       })

        object_types = {}
        for column in self.data.select_dtypes(include='object').columns:
          unique_types = self.data[column].apply(lambda x: type(x) if x is not None else float).unique()
          object_types[column] = [i for i in unique_types]
        pivot_info['object_types'] = pivot_info.index.map(lambda col: object_types.get(col, None))

        pivot_info = pivot_info[['count', 'null', 'type', 'object_types', 'unique', 'unique_values']]

        print(f'Shape of the data: {self.data.shape}')
        return pivot_info



# dataset = InfoDataSet("////", data)
# dataset.pivot_describe()
#

# очистка от "мусора" признаков, содержащих инфу о денежных средствах
def clean_currency_column(value):
    value = re.sub(r'[€]', '', str(value))
    value = re.sub(r'\s+', '', value)
    value = value.replace('.', '')
    value = value.replace(',', '.')
    return float(value)

"""# ***Part 1.1. Data cleaning and processing***"""

data_calls = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Calls (Done).xlsx', dtype={'Id': str, "CONTACTID": str})
data_contacts = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Contacts (Done).xlsx', dtype={'Id': str})
data_deals = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Deals (Done).xlsx', dtype={'Id': str, 'Contact Name': str})
data_spend = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Spend (Done).xlsx')

"""## ***1.***   *Calls*"""

# data_calls = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Calls (Done).xlsx', dtype={'Id': str, "CONTACTID": str})

"""### *Первичный обзор данных*"""

display(data_calls.head(2))

info_calls = InfoDataSet("Calls_info исходная", data_calls)
pivot_info_calls = info_calls.pivot_info()
display(pivot_info_calls)

"""### *Удаление дубликатов и неактуальных столбцов*

----------------------------------------
✅ Dialled Number (Набранный номер телефона) и Tag (Тэг вызова) абсолюино пустые - удаляю.
"""

data_calls.drop(['Dialled Number', 'Tag'], axis=1, inplace=True)

data_calls.shape

"""----------------------------------------
✅ проверка строк на дубликаты (без id) и их удаление. Учитываем, что один и тот же менеджер не может звонить в одно и тоже время нескольким лицам и долго говорить. Поэтому ищем дубликаты при условии, что Call Duration (in seconds) превышает 50 секунд
"""

duplicates = data_calls[(data_calls['Call Duration (in seconds)'] > 50) & data_calls.duplicated(subset=data_calls.columns[1:])]
print(duplicates.shape)
duplicates.head(2)

data_calls = data_calls.drop(index=duplicates.index)

data_calls.shape

"""### *Обработка пропусков и проверка типов данных по каждому признаку*"""

info_calls_2 = InfoDataSet("Calls_info после удаления дубликатов и неактуальных строк", data_calls)
pivot_info_calls_2 = info_calls_2.pivot_info()
display(pivot_info_calls_2)

"""----------------------------------------
✅ ***Id*** - Уникальный идентификатор для каждого звонка.

Всё ок, пропусков нет, тип данных str удобна для id

----------------------------------------
✅ ***Call Start Time*** Время начала звонка.

Меняю тип данных на datetime
"""

data_calls["Call Start Time"] = pd.to_datetime(data_calls["Call Start Time"], errors="raise")

"""----------------------------------------
✅ ***Call Owner Name*** - Имя лица, ответственного за звонок.

Всё ок, пропусков нет, 33 уникальных значения, тип данных object/st

----------------------------------------
***✅CONTACTID*** - Уникальный идентификатор контакта.

Всё ок, пропусков много, но их не убираем, тип данных str удобна для id

✔: имеющиеся пропуски не удаляю - на аналитику влиять не будут (они не будут соединятся при join)

----------------------------------------
✅ ***Call Type*** - Тип звонка (Входящие, Исходящие, Пропущенные).

Всё ок, пропусков нет, 3 уникальных значения, тип данных object/str

----------------------------------------
✅ ***Call Duration (in seconds)*** - Длительность звонка в секундах.

Имеет тип данных float64. Есть пропущенные значения (83), которые тоже имеют тип данных float64, так как это nan значения

все nan - 83 звонка были запланированы CRM системой, и звонок по какой-то причине не состоялся:
"""

data_calls['Call Status'][data_calls['Call Duration (in seconds)'].isna()].unique()

"""'Cancelled', 'Overdue', 'Scheduled' = "Отменено", "Просрочено", "Запланировано"
"""

data_calls['Outgoing Call Status'][data_calls['Call Duration (in seconds)'].isna()].unique()

"""Создаю дополнительный столбец "Flag_Call Duration (in seconds)" с инфой в каких сроках были значения nan (там будет 1), а затем на место nan ставлю 0 и меняю тип данных на int"""

data_calls['Flag_Call Duration (in seconds)'] = data_calls['Call Duration (in seconds)'].apply(lambda x: 1 if pd.isna(x) else 0)
data_calls['Call Duration (in seconds)'] = data_calls['Call Duration (in seconds)'].fillna(0).astype(int)

"""----------------------------------------
✅ ***Call Status*** - Окончательный статус звонка.

Всё ок, пропусков нет, 11 уникальных значения, тип данных object

----------------------------------------
✅ ***Outgoing Call Status*** - Статус исходящих вызовов.

Тип данных object/str + float из-за nan значений. 4 уникальных значения.

nan связаны с тем, что эти звонки являются входящими, а не исходящими, поэтому не могут иметь никакого статуса

----------------------------------------
✅ ***Scheduled in CRM*** - указывает был ли звонок запланирован через систему CRM.

Количество пропусков совпадает с Outgoing Call Status (там столько же nan). Получается, что nan - это входящие звонки, которые не могут быть запланированы CRM системой

Имеет два варианта значений: 0 и 1. Это "нет" и "да" - должен быть тип данных bool. Также есть значения nan.

Тип данных float64.
"""

data_calls['Scheduled in CRM'].value_counts()

# Создаю дополнительный столбец Flag_Scheduled in CRM с инфой в каких сроках были значения nan (там будет 1),
# а затем на место nan ставлю 0 и меняю тип данных на int
data_calls['Flag_Scheduled in CRM'] = data_calls['Scheduled in CRM'].apply(lambda x: 1 if pd.isna(x) else 0)
data_calls['Scheduled in CRM'] = data_calls['Scheduled in CRM'].fillna(0).astype(int)

info_calls_3 = InfoDataSet("Calls_info итоговое", data_calls)
pivot_info_calls_3 = info_calls_3.pivot_info()
display(pivot_info_calls_3)

"""## ***2.***   *Contacts*


"""

# data_contacts = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Contacts (Done).xlsx', dtype={'Id': str})

"""### *Первичный обзор данных*"""

display(data_contacts.head(2))

info_contacts = InfoDataSet("Contacts_info исходная", data_contacts)
display(info_contacts.pivot_info())

"""### *Удаление дубликатов и неактуальных столбцов*

----------------------------------------
✅ проверка строк на дубликаты (без id) и их удаление, при условии, что Created Time равно Modified Time
"""

duplicates = data_contacts[(data_contacts['Created Time'] == data_contacts['Modified Time']) & data_contacts.duplicated(subset=data_contacts.columns[1:])]
print(duplicates.shape)
duplicates.head(2)

data_contacts = data_contacts.drop(index=duplicates.index)

data_contacts.shape

"""### *Обработка пропусков и проверка типов данных по каждому признаку*"""

info_contacts_2 = InfoDataSet("Сontacts_info после удаления дубликатов и неактуальных строк", data_contacts)
pivot_info_contacts_2 = info_contacts_2.pivot_info()
display(pivot_info_contacts_2)

"""✅ ***Id:*** Идентификатор контакта - всё ок

✅ ***Contact Owner Name:*** Имя лица, ответственного за управление контактом - пропусков нет, но тип данных object имеет два подтипа - str и bool.Там должны содержаться имена, но где-то есть True или False.
"""

data_contacts[data_contacts['Contact Owner Name'].isin([True, False])]

"""всего одна строка содержит False. Учитывая, что их там 18548 - удаляю"""

data_contacts = data_contacts[data_contacts['Contact Owner Name'] != False]

"""✅ ***Created Time:*** Дата внесения контакта в базу.

✅ ***Modified Time:*** Дата последней модификации контакта.

Оба столбца пропусков не имеют, тип данных object/str, хотя содержат информацию о дате и времени, поэтому перевожу в формат datetime
"""

data_contacts["Created Time"] = pd.to_datetime(data_contacts["Created Time"], errors="raise")

data_contacts["Modified Time"] = pd.to_datetime(data_contacts["Modified Time"], errors="raise")

info_contacts_3 = InfoDataSet("Contacts_info итоговое", data_contacts)
pivot_info_contacts_3 = info_contacts_3.pivot_info()
display(pivot_info_contacts_3)

"""## ***3.***   *Deals*"""

# data_deals = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Deals (Done).xlsx', dtype={'Id': str, 'Contact Name': str})

"""### *Первичный обзор данных*



"""

display(data_deals.head(2))

info_deals_1 = InfoDataSet("Deals_info исходная", data_deals)
pivot_info_deals = info_deals_1.pivot_info()
display(pivot_info_deals)

"""### *Удаление дубликатов и неактуальных столбцов*

----------------------------------------
✅ проверка строк на дубликаты (без id) и их удаление
"""

data_deals[data_deals.duplicated(subset=data_deals.columns[1:])].shape

data_deals[data_deals.duplicated(subset=data_deals.columns[1:])].head(2)

data_deals.drop_duplicates(subset=data_deals.columns[1:], inplace=True)

data_deals[data_deals.duplicated(subset=data_deals.columns[1:])].shape

"""----------------------------------------
✅ инфа от разработчика данных:

Lost Reason содержит Duplicate - эти строки следует удалить
"""

data_deals[data_deals['Lost Reason'] == 'Duplicate'].head(1)

data_deals[data_deals['Lost Reason'] == 'Duplicate'].shape

data_deals = data_deals[data_deals['Lost Reason'] != 'Duplicate']

data_deals[data_deals['Lost Reason'] == 'Duplicate'].shape

"""----------------------------------------
✅ инфа от разработчика данных:

Source содержит Test - эти строки следует удалить
"""

data_deals[data_deals['Source'] == 'Test'].head(1)

data_deals[data_deals['Source'] == 'Test'].shape

data_deals = data_deals[data_deals['Source'] != 'Test']

data_deals[data_deals['Source'] == 'Test'].shape

"""----------------------------------------
✅ ***Id:*** Уникальный идентификатор для каждой сделки.

Имеет 2 пропуска. Тип данных object/видами str и float(из-за NaN).

Тип данных не меняю. Удаляю 2 строки с пропусками (во всех столбцах значения NaN)

"""

data_deals[data_deals['Id'].isna()]

data_deals = data_deals.dropna(subset=['Id'])

"""----------------------------------------
✅ ***Contact Name***: Идентификатор контактного лица по сделке.


Пропусков 47 (осталось после удаления тестовых данных и дубликатов), тип данных object/str и float (из-за NaN). Пропуски заполнить не сможем, поэтому эти строки удаляю (непонятно откуда они появились и реальные ли это данные).


"""

data_deals = data_deals.dropna(subset=['Contact Name'])

"""### *Обработка пропусков и проверка типов данных по каждому признаку*"""

info_deals_2 = InfoDataSet("Deals_info после удаления дубликатов и неактуальных строк", data_deals)
pivot_info_deals_2 = info_deals_2.pivot_info()
display(pivot_info_deals_2)

"""----------------------------------------
✅ ***Deal Owner Name:*** Имя лица, ответственного за сделку.

Имеет 28 nan. Тип данных object\str и float(из-за NaN).

Пропуски вначале заполняю по полю Contact Name - если известно какой менеджер хоть один раз работал с этим клиентом, то по умолчанию ставлю для того же Contact Name того же менеджера. Затем оставшиеся пропуски заполняю 'Unknown'



"""

mode_values = data_deals.groupby('Contact Name')['Deal Owner Name'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)
data_deals['Deal Owner Name'] = data_deals['Contact Name'].map(mode_values)

data_deals['Deal Owner Name'].isna().sum()

"""Получилось 6 пропусков заполнить. Остальные перевожу в Unknown"""

data_deals['Deal Owner Name'] = data_deals['Deal Owner Name'].fillna('Unknown')

"""----------------------------------------
✅ ***Created Time:*** Метка времени, когда была создана сделка.

Пропусков нет, класс object/str. Перевожу в datetime64
"""

data_deals["Created Time"] = pd.to_datetime(data_deals["Created Time"], errors="coerce")

"""----------------------------------------
✅ ***Closing Date:*** Дата закрытия сделки, если применимо.

Пропусков много - 6602, класс object/str и float (из-за NaN). Перевожу в datetime64

✔: пропуски заполнять не надо, это невозможно, nan на расчеты не повлияют

"""

data_deals["Closing Date"] = pd.to_datetime(data_deals["Closing Date"], errors="coerce")

"""----------------------------------------
***Проверка хронологии в Created Time и Closing Date***

В Closing Date во многих строках стоит время 0 часов 0 минут (скорее всего проставляется автоматически) - в итоге 2380 строк, где Closing Date раньше, чем Created Time. Если убрать фактор часов и минут, то получим 37 строки. Их и буду менять местами.

Остальные строки (из 2380) - дата открытия и закрытия совпадают, отличается только время
"""

# оставляем фактор часов и минут
data_deals.loc[data_deals['Closing Date'] < data_deals['Created Time']].shape

# убрали влияние фактора часов и минуты
data_deals.loc[data_deals['Closing Date'].dt.date < data_deals['Created Time'].dt.date].shape

data_deals[['Closing Date', 'Created Time']].loc[data_deals['Closing Date'].dt.date < data_deals['Created Time'].dt.date].head(1)

maska = data_deals['Closing Date'].dt.date < data_deals['Created Time'].dt.date
temp_columns = data_deals.loc[maska, 'Closing Date']

data_deals.loc[maska, 'Closing Date'] = data_deals.loc[maska, 'Created Time']
data_deals.loc[maska, 'Created Time'] = temp_columns

data_deals[['Closing Date', 'Created Time']].iloc[427]

"""----------------------------------------
✅  ***Quality***:  Классификация качества сделки, указывающая на ее потенциальный или целевой статус.

Пропусков много, класс object/str и float (из-за NaN). Всего 5 уникальных значений.

Восполнить NaN невозможно, поэтому пропуски заполняю 'Unknown'

"""

data_deals['Quality'] = data_deals['Quality'].fillna('Unknown')

"""----------------------------------------
✅ ***Stage***: Текущая стадия сделки.

Пропусков нет, тип данных object/str. Всего 13 уникальных значений.

"""

# data_deals['Stage'].unique()

"""----------------------------------------
✅ ***Lost Reason***: Причина, по которой сделка была потеряна, если применимо.

Пропусков много, тип данных object/str и float (из-за NaN). Уникальных значений - 20.

Оставляю NaN, так как не все сделки могут быть потеряны, соответственно никакого статуса здесь иметь не будут

----------------------------------------
✅ ***Page***: Веб-страница или целевая страница, на которой был получен лид.

Пропусков нет, тип данных object/str, 31 уникальное значение

----------------------------------------
✅ ***Campaign***: Название или код маркетинговой кампании, связанной со сделкой.

Пропусков много - 4199, класс object/str и float (из-за NaN). И много уникальных значений - 152.


Пропуски заполняю 'Unknown'
"""

data_deals['Campaign'] = data_deals['Campaign'].fillna('Unknown')

"""----------------------------------------
✅ ***SLA***: Время действия соглашения об уровне обслуживания, указывающее на время отклика. То есть это время с момента получения заявки и до созвона

Пропусков много - 4804. Тип данных object, который содержит в себе три разных подтипа - float, datetime.time и datetime.timedelta.

float - это nan значения, которые оставлю nan, остальные типы данных переведу в секунды, минуты и часы. Результат сохраняю в новые столбцы. А оригинальный столбец SLA удаляю



"""

pivot_info_deals_2['object_types']['SLA']

def convert_to_seconds(value):
    if pd.isna(value):
        return np.nan

    elif isinstance(value, time):
        return value.hour * 3600 + value.minute * 60 + value.second

    elif isinstance(value, timedelta):
        return value.total_seconds()

data_deals['SLA_seconds'] = data_deals["SLA"].apply(convert_to_seconds)
# data_deals['SLA_seconds'].value_counts()

def convert_to_minutes(value):
    if pd.isna(value):
        return np.nan

    elif isinstance(value, time):
        return (value.hour * 60) + value.minute + (value.second / 60)

    elif isinstance(value, timedelta):
        return value.total_seconds() / 60

data_deals['SLA_minutes'] = data_deals["SLA"].apply(convert_to_minutes)
# data_deals['SLA_minutes'].value_counts()

def convert_to_hours(value):
    if pd.isna(value):
        return np.nan

    elif isinstance(value, time):
      hour1 = round(value.hour + (value.minute / 60) + (value.second / 3600), 2)
      return hour1

    elif isinstance(value, timedelta):
      hour2 = round(value.total_seconds() / 3600, 2)
      return hour2

data_deals['SLA_hours'] = data_deals["SLA"].apply(convert_to_hours)
# data_deals['SLA_hours'].value_counts()

data_deals = data_deals.drop(columns=['SLA'])

"""----------------------------------------
✅ ***Content  (=Ad):*** Конкретная реклама, показываемая пользователям.

Пропусков много - 6021, класс object/str и float (из-за NaN). И много уникальных значений - 179.

Пропуски перевожу в Unknown

"""

data_deals['Content'] = data_deals['Content'].fillna('Unknown')

"""----------------------------------------
✅ ***Term*** ***(=AdGroup):*** Подмножество в кампании, содержащее одно или несколько объявлений с одинаковыми целями или настройками.

Пропусков много - 7717, класс object/str и float (из-за NaN). И много уникальных значений - 214.

Пропуски перевожу в Unknown

"""

data_deals['Term'] = data_deals['Term'].fillna('Unknown')

"""----------------------------------------
✅ ***Source***: Источник лида.

Пропусков нет, тип данных object/str. Всего 12 уникальных значений.

----------------------------------------
✅ ***Payment Type***: Тип используемого или ожидаемого способа оплаты.


Пропусков много, так как это данные о проданных курсах (удачных лидах) или о незакрытых сделках, класс object/str и float (из-за NaN). Всего 3 уникальных значений.

----------------------------------------
✅ ***Product***: Конкретный продукт или услуга, связанная со сделкой.


Пропусков много, так как это данные о проданных курсах (удачных лидах) или о незакрытых сделках, класс object/str и float (из-за NaN). Всего 5 уникальных значений.

Пропуски оставляю ( - это потерянные лиды или незакрытые сделки)
"""

data_deals['Product'].value_counts()

"""Продукты Find yourself in IT и Data Analytics имеют всего 4 строки данных (причина не известна - может от них отказались, не набрав студентов, или только начали набор). Это количество не имеет статистического значения, поэтому их удаляю"""

data_deals = data_deals[~data_deals['Product'].isin(['Find yourself in IT', 'Data Analytics'])]

"""----------------------------------------
✅ ***Education Type***: Тип образования или обучения.


Пропусков много, так как это данные о проданных курсах (удачных лидах) или о незакрытых сделках, класс object/str и float (из-за NaN). Всего 2 уникальных значения - Morning, Evening

----------------------------------------
✅ ***Course duration***: Длительность курса, на который поступает студент


Пропусков много - 16148, класс float64. Имеет всего 2 уникальных значения 6.0 и 11.0.

Проверила, что если известен Product, то Course duration также известен. Все нан меняю на 0 и тип данных на int
"""

data_deals['Course duration'] = data_deals['Course duration'].fillna(0).astype(np.int32)

"""----------------------------------------
✅ ***Months of study***: Количество месяцев, которые отучился студент

Пропусков много - 18826, класс float64, уникальных 12 значений (максимальная длительность обучения 11 месяцев + 0) Пропуски заменяю на 0, так как в этом случае человек не учился (либо отказался, либо не начал обучение), и переведу тип данных в int
"""

data_deals['Months of study'] = data_deals['Months of study'].fillna(0).astype(np.int32)

"""----------------------------------------
✅ ***Initial Amount Paid***: Первоначальный платеж клиента - сколько должен внести, но не факт, что внёс.

Пропусков много - 15621, тип данных object/str и float (из-за NaN).

Необходимо перевести во float, так как это денежные средства. Чтобы это сделать, вначале надо очистить значения от лишних символов (clean_currency_column - вспомогательная функция вначале), а затем все  NaN переведём в 0. Сохраняю всё это в новый столбец.
"""

data_deals['InAmountPaid cleaned'] = data_deals['Initial Amount Paid'].apply(clean_currency_column)
data_deals['InAmountPaid cleaned'].fillna(0, inplace=True)

"""----------------------------------------
✅ ***Offer Total Amount***: Общая сумма предложения, представленного клиенту.


Пропусков много - 15601, тип данных object/str и float (из-за NaN).

Необходимо перевести во float, так как это денежные средства. Чтобы это сделать, вначале надо очистить значения от лишних символов (clean_currency_column - вспомогательная функция вначале), а затем все  NaN переведём в 0. Сохраняю всё это в новый столбец.
"""

data_deals['OfTotAmount cleaned'] = data_deals['Offer Total Amount'].apply(clean_currency_column)
data_deals['OfTotAmount cleaned'].fillna(0, inplace=True)

"""----------------------------------------
***Проверка корректности заполнения сумм***

Сумма в Initial Amount Paid должна быть меньше, чем в Offer Total Amount, поскольку это первоначальный платеж (он не может быть больше всей стоимости курса). Там где наоборот - меняем местами, так как это или ошибка при заполнении менеджером, или ошибка системы.

Проверку выполняем по уже очищенным столбцам. Оригинальные столбцы удаляю
"""

mask = data_deals['OfTotAmount cleaned'] < data_deals['InAmountPaid cleaned']
temp_columns = data_deals.loc[mask, 'OfTotAmount cleaned']

data_deals.loc[mask, 'OfTotAmount cleaned'] = data_deals.loc[mask, 'InAmountPaid cleaned']
data_deals.loc[mask, 'InAmountPaid cleaned'] = temp_columns

data_deals [['InAmountPaid cleaned', 'OfTotAmount cleaned']].loc[data_deals['OfTotAmount cleaned'] != 0].head()

"""Оригинальные столбцы удаляю"""

data_deals.drop(['Initial Amount Paid', 'Offer Total Amount'], axis=1, inplace=True)

# print(data_deals.columns)

"""----------------------------------------
✅ ***City***: Город, относящийся к клиенту.

Пропусков очень много - 17138, тип данных object/str и float (из-за NaN). 874 уникальных - нет смысла переводить в категории.

Тип данных оставляю неизменным.

Поскольку место жительство люди меняют не часто, то пробую заполнить пропуски по Contact Name - они повторяются (с одними и теми же людьми менеджеры общаются несколько раз)

"""

mode_values = data_deals.groupby('Contact Name')['City'].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)
data_deals['City'] = data_deals['Contact Name'].map(mode_values)

data_deals['City'].isna().sum()

"""Количество пропусков в "City" сократилось с 17138 до 16519

----------------------------------------
✅ ***Level of Deutsch***: Уровень владения немецким языком клиента, если применимо.

Пропусков очень много - 18390, тип данных object/str и float (из-за NaN). 214 уникальных - нет смысла переводить в категории.

Уровень владения немецким языком меняется нечасто, поэтому пробую заполнить пропуски по Contact Name.
"""

mode_values = data_deals.groupby('Contact Name')['Level of Deutsch'].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)
data_deals['Level of Deutsch'] = data_deals['Contact Name'].map(mode_values)

data_deals['Level of Deutsch'].isna().sum()

"""Количество пропусков в "Level of Deutsch" сократилось"""

info_deals_3 = InfoDataSet("Spend_info итоговое", data_deals)
pivot_info_deals_3 = info_deals_3.pivot_info()
display(pivot_info_deals_3)

"""##  ***4.***   *Spend*"""

# data_spend = pd.read_excel('/content/drive/MyDrive/PROJECT    COMPLEX/Data source/Spend (Done).xlsx')

"""### *Первичный обзор данных*"""

display(data_spend.head(2))

info_spend_1 = InfoDataSet("Spend_info исходная", data_spend)
pivot_info_spend_1 = info_spend_1.pivot_info()
display(pivot_info_spend_1)

"""### *Удаление дубликатов и неактуальных столбцов*

----------------------------------------
✅ проверка строк на дубликаты (без id) и их удаление
"""

data_spend[data_spend.duplicated(subset=data_spend.columns[1:])].shape

data_spend[data_spend.duplicated(subset=data_spend.columns[1:])].head(2)

data_spend.drop_duplicates(subset=data_spend.columns[1:], inplace=True)

data_spend[data_spend.duplicated(subset=data_spend.columns[1:])].shape

"""----------------------------------------
✅ инфа от разработчика данных:

Source содержит Test - эти строки следует удалить
"""

data_spend[data_spend['Source'] == 'Test'].head(1)

data_spend[data_spend['Source'] == 'Test'].shape

data_spend = data_spend[data_spend['Source'] != 'Test']

data_spend[data_spend['Source'] == 'Test'].shape

"""### *Обработка пропусков и проверка типов данных по каждому признаку*"""

info_spend_2 = InfoDataSet("Spend_info после удаления дубликатов и неактуальных строк", data_spend)
pivot_info_spend_2 = info_spend_2.pivot_info()
display(pivot_info_spend_2)

"""----------------------------------------
✅***Date:***   Дата, указывающая, когда были отслежены показы, клики и расходы на рекламу.

Всё ок. Тип данных datetime64, пропусков нет

----------------------------------------
✅***Source:***   Канал, на котором было показано объявление

Пропусков нет, тип данных object/str, 13 уникальных значений
"""

# data_spend['Source'].unique()

"""----------------------------------------
✅***Campaign:***   Кампания, в рамках которой было показано объявление.

Пропусков 1252. Тип данных object/str и float (из-за nan). Имеет 50 уникальных значений

Пропуски заполняю 'Unknown'
"""

data_spend['Campaign'] = data_spend['Campaign'].fillna('Unknown')

"""----------------------------------------
✅***Impressions:***   Количество показов рекламы пользователям.

Всё ок, пропусков нет, тип данных int64

----------------------------------------
✅***Spend:***   Количество денег, потраченных на рекламную кампанию или группу объявлений за указанный период.

Всё ок, пропусков нет, тип данных float64 (не целые числа в подсчете затрат)

----------------------------------------
✅***Clicks:***   Количество нажатий пользователей на рекламу.

Всё ок, пропусков нет, тип данных int64

----------------------------------------
✅***AdGroup:***   Подмножество в кампании, содержащее одно или несколько объявлений с одинаковыми целями или настройками (целевая аудитория).

Пропусков много (2086), тип данных object/str и float (из-за nan). Уникальных значений 24.
"""

# data_spend.loc[data_spend['AdGroup'].isna()] - действительно Unknown, на первый взгляд не видно от чего зависит

data_spend['AdGroup'] = data_spend['AdGroup'].fillna('Unknown')

"""----------------------------------------
✅***Ad:***   Конкретная реклама, показываемая пользователям.

Пропусков много (2086) - столько же как и в AdGroup. Эти столбцы зависят между собой, так как nan совпадают, но не понятно от каких признаков зависят оба эти столбца.

Тип данных object/str и float (из-за nan). Уникальных значений 165.

Неизвестные (nan) перевожу в Unknown


"""

data_spend['Ad'] = data_spend['Ad'].fillna('Unknown')

info_spend_3 = InfoDataSet("Spend_info итоговое", data_spend)
pivot_info_spend_3 = info_spend_3.pivot_info()
display(pivot_info_spend_3)



"""# ***Part 1.2. Descriptive statistics***

## *Рассчитайте сводную статистику (среднее значение, медиана, режим, диапазон) для числовых полей.*

### data_calls
"""

data_calls.describe()

data_calls[data_calls['Call Duration (in seconds)'] == data_calls['Call Duration (in seconds)'].max()]

for column in data_calls.select_dtypes([int, float]).columns:
    print("--" * 30, column, "--" * 30)
    plt.figure(figsize = (8, 2))
    plt.subplot(1, 2, 1)
    data_calls[column].hist(grid=False)
    plt.ylabel('count')
    plt.subplot(1, 2, 2)
    sns.boxplot(x=data_calls[column])
    plt.show()

"""По факту в этой таблице всего один числовой столбец - Call Duration (in seconds). Остальные имеют значения 0 или 1, поэтому их графики выглядят как графики категориальных переменных

### data_contacts
"""

data_contacts.describe()

"""Здесь нет числовых переменных, только datetime64

### data_deals
"""

data_deals.describe()

for column in data_deals.select_dtypes([int, float]).columns:
    print("--" * 20, column, "--" * 20)
    plt.figure(figsize = (8, 2))
    plt.subplot(1, 2, 1)
    data_deals[column].hist(grid=False)
    plt.ylabel('count')
    plt.subplot(1, 2, 2)
    sns.boxplot(x=data_deals[column])
    plt.show()

"""Полноценными числовыми переменными являются только переменные SLA (все варианты), InAmountPaid cleaned и OfTotAmount cleaned. Остальные выглядят как категориальные

### data_spend
"""

data_spend.describe()

for column in data_spend.select_dtypes([int, float]).columns:
    print("--" * 20, column, "--" * 20)
    plt.figure(figsize = (8, 2))
    plt.subplot(1, 2, 1)
    data_spend[column].hist(grid=False)
    plt.ylabel('count')
    plt.subplot(1, 2, 2)
    sns.boxplot(x=data_spend[column])
    plt.show()

data_spend[data_spend['Impressions'] == data_spend['Impressions'].max()]

"""В Impressions есть один выброс - 431445 показов рекламы, при этом стоимость такой рекламы не самая высокая (максимальный спенд 774, а здесь всего 236.1). Количество кликов высокое. При этом источником такой рекламы является Google Ads - допустимо ли для этого источника такая стоимость с таким показом надо уточнять.

### Совместимость временных рядов по всем таблицам

во всех таблицах есть временные признаки. Необходима проверка на совместимость - все должны быть за один и тот же период.
"""

print(data_calls['Call Start Time'].min(), data_calls['Call Start Time'].max())
print(data_contacts['Created Time'].min(), data_contacts['Created Time'].max())
print(data_deals['Created Time'].min(), data_deals['Created Time'].max())
print(data_spend['Date'].min(), data_spend['Date'].max())

"""Во всех 4х таблицах последняя дата совпадает, а вот первоначальные разные. Чтобы временные ряды были совместимы, выберем самую познюю из первоначальных дат и обрежем данные в 3х таблицах."""

start_max_date = max(data_calls['Call Start Time'].min(), data_contacts['Created Time'].min(), data_deals['Created Time'].min(), data_spend['Date'].min())
start_max_date

data_calls = data_calls[data_calls['Call Start Time'] >= start_max_date]
data_contacts = data_contacts[data_contacts['Created Time'] >= start_max_date]
data_deals = data_deals[data_deals['Created Time'] >= start_max_date]

# print(data_calls['Call Start Time'].min(), data_calls['Call Start Time'].max())
# print(data_contacts['Created Time'].min(), data_contacts['Created Time'].max())
# print(data_deals['Created Time'].min(), data_deals['Created Time'].max())
# print(data_spend['Date'].min(), data_spend['Date'].max())

"""## *Анализируйте категориальные поля, такие как качество, стадия, источник и продукт.*

### data_calls
"""

data_calls.describe(include=['object'])

"""Outbound = Исходящий / Attended Dialled =: Набранный номер / Completed = Завершен

С учетом того, что в таблице почти 96 тысяч записей, я не вижу здесь никаких странностей. Много исходящих, много набранных номеров - менеджеры работают активно. Завершенными являются почто все - что тоже неплохо, менеджеры полностью обрабатывают заявки.

### data_contacts
"""

data_contacts.describe(include=['object', 'category'])

"""Charlie Davis имеет очень много повторений. Сравним с другими менеджерами, чтобы понять много или мало"""

aggregated_data = data_contacts.groupby('Contact Owner Name').agg({'Id': 'count','Created Time': 'min','Modified Time': 'max'})
sorted_data = aggregated_data.sort_values(by='Id', ascending=False)

print(sorted_data.head())

"""Charlie Davis работает давно и его показатели выше, чем у других, но не намного. Менеджер на втором месте отстает всего на 200 записей, глядя на разницу между менеджерами - это может быть норм . Важно посмотреть сколько рабочих дней у каждого, чтобы сделать окончательный вывод

### data_deals
"""

data_deals.describe(include=['object', 'category'])

"""Здесь много категориальных переменных, по многим из которых стоит Unknown. Важность каждой категории надо исследовать в процессе анализа по сферам деятельности. Если сейчас отсеять по каждому признаку значения, которые на первый взгляд "неинформативны", то может произойти излишняя потеря данных. Поэтому отсекать буду только в процессе конкретного анализа."""



"""### data_spend"""

data_spend.describe(include=['object', 'category'])

"""Все 4 поля можно считать категориальными - количество уникальных значительно меньше общего количества

# ***Сохранение результатов***
"""

df = {'calls_clear': data_calls, 'contacts_clear': data_contacts, 'deals_clear': data_deals, 'spend_clear': data_spend}

with open('data_1.pkl', 'wb') as f:
    pickle.dump(df, f)

with open('data_1.pkl', 'rb') as f:
    data_1 = pickle.load(f)

data_1.keys()

calls_clear, contacts_clear, deals_clear, spend_clear  = data_1.values()

